{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose of notebook:\n",
        "\n",
        "This notebook is designed to pull down test TIU formatted notes from Azure storage so that the research/development instance of the codebase can be run. To run this notebook, you can run each cell until the end. \n",
        "\n",
        "### Errors:\n",
        "If you run into any errors, most of the times the solution is just to try again or stop the kernel and restart. In rare cases where even that doesn't work, you can try to delete this notebook and get a fresh copy from within Suzanne Tamang's user folder. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "main_start = time.time()\n",
        "\n",
        "# end = time.time()\n",
        "\n",
        "print('time at stage {:.6}'.format(time.time() - main_start))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "ae14eb3c-7691-4c6c-82aa-623abfc0ca9a",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793820755
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "d13fb852-4054-4061-a855-d7847db9a6c8",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793821467
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg, to_timestamp, size\n",
        "from pyspark.sql.functions import substring, regexp_replace, trim, row_number, monotonically_increasing_id, rand\n",
        "from pyspark.sql.functions import concat_ws, expr, lit, regexp_extract, collect_list, explode\n",
        "from pyspark.sql.functions import sum as Fsum\n",
        "# from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import IntegerType, BooleanType\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window as W\n",
        "# from pyspark.sql.types import *\n",
        "from functools import reduce  # For Python 3.x\n",
        "# from pyspark.sql import DataFrame\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "from pyspark.sql.types import ArrayType, IntegerType\n",
        "import azureml.core\n",
        "from azureml.core import Workspace, Datastore, Dataset, Keyvault\n",
        "from azureml.contrib.dataset import FileHandlingOption\n",
        "from azureml.core import Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "from azureml.widgets import RunDetails\n",
        "from azureml.core import ScriptRunConfig, Environment\n",
        "from azureml.core.runconfig import MpiConfiguration\n",
        "from azureml.core import Environment\n",
        "from azureml.core import Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.telemetry import set_diagnostics_collection\n",
        "import azureml.core\n",
        "import math"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793821780
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg, to_timestamp, size\n",
        "from pyspark.sql.functions import substring, regexp_replace, trim, row_number, monotonically_increasing_id, rand\n",
        "from pyspark.sql.functions import concat_ws, expr, lit, explode\n",
        "from pyspark.sql.functions import sum as Fsum\n",
        "# from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import IntegerType, BooleanType\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Window as W\n",
        "# from pyspark.sql.types import *\n",
        "from functools import reduce  # For Python 3.x\n",
        "# from pyspark.sql import DataFrame\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
        "# from pyspark.sql import Window"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "361fdbd8-1007-4c68-b4c8-57910ba3e208",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793821966
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.types import ArrayType, IntegerType, StringType"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "98beb54e-6248-4d8e-a5db-6f3eb65c17c6",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793822169
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, date_trunc\n",
        "from pyspark.sql.functions import  min as min_\n",
        "from pyspark.sql.functions import  max as max_\n",
        "from pyspark.sql.functions import  avg as avg_"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "847b8929-1733-49a5-b700-13736292cf22",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793822363
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws, expr, lit, regexp_extract, collect_list, explode\n",
        "import math\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf, struct\n",
        "from pyspark.sql.functions import udf, explode\n",
        "from pyspark.sql.functions import row_number, monotonically_increasing_id, flatten\n",
        "from pyspark.sql.types import ArrayType, IntegerType"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "97622108-93cc-4853-86d7-e271168863af",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793822648
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "import re\n",
        "re.sub(r'a', 'b', 'banana')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "7771c823-0afd-45ca-8e5d-b29ab02372b2",
          "showTitle": false,
          "title": ""
        },
        "gather": {
          "logged": 1684793822945
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYSPARK_DRIVER_PYTHON'] = '/anaconda/envs/azureml_py38/bin/python'\n",
        "os.environ['PYSPARK_PYTHON'] = '/anaconda/envs/azureml_py38/bin/python'\n",
        "os.environ['RSLEX_DIRECT_VOLUME_MOUNT'] = 'true'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793823530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Datastore, Dataset \n",
        "from azureml.data.datapath import DataPath\n",
        "from pathlib import Path\n",
        "from azureml.core.authentication import AzureCliAuthentication \n",
        "import tempfile\n",
        "import os\n",
        "import shutil\n",
        "# retrieve Azure workspace config settings. The first time this is run on a compute instance, you will need to authenticate through terminal (run 'az login') which will prompt you to authenticate through browser/code.\n",
        "# use this is if 'ws = Workspace.from_config() is not working below\n",
        "cli_auth = AzureCliAuthentication()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793823706
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import AzureCliAuthentication \n",
        "\n",
        "cli_auth = AzureCliAuthentication()\n",
        "\n",
        "!az login"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1684793871299
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "!az login"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ws = Workspace.from_config()\n",
        "# my_secret = os.environ.get(\"MY_SECRET\")\n",
        "ws = Workspace.from_config(auth=cli_auth)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793871499
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rawzone = Datastore.get(ws, datastore_name='rawzonedata')\n",
        "workspacezone = Datastore.get(ws, datastore_name='workspaceblobstore')\n",
        "# dssbzone = Datastore.get(ws, datastore_name='dssbzonefs')\n",
        "print('time at stage {:.6}'.format(time.time() - main_start))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793871868
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datastores = ws.datastores\n",
        "for name, datastore in datastores.items():\n",
        "    print(name, datastore.datastore_type)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793872234
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delta_rootUrl = '/tmp/workspacezone'\n",
        "delta_datasets = 'TIU_202204to202209_visit_document_note_noXPNA2'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793872419
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ds = Dataset.File.from_files(path= \\\n",
        "    [DataPath(datastore=workspacezone, path_on_datastore=\n",
        "              (delta_datasets))])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793875327
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mount_context = dataset_ds.mount(mount_point=delta_rootUrl)\n",
        "mount_context.start()  # this will mount the file streams"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793900437
        }
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "!sudo umount /tmp/workspacezone"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "mount_context.stop() "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ls /tmp/workspacezone/metadata"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901186
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1666067510549
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls /tmp/workspacezone/tsv/*"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1684793901203
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run all above here, below is how to concat loop upload "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ls /tmp/workspacezone/metadata/lastmodifieddatetime_month=202205"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "gather": {
          "logged": 1684793901218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901235
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filelist = glob.glob('/tmp/workspacezone/metadata/*.txt')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901251
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputFoldermetadata = '/tmp/visit_document_note/metadata'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901269
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/tmp/visit_document_note/metadata')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901287
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $outputFoldermetadata"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for count, file in enumerate(filelist):\n",
        "    filename = file.split('/')[4]\n",
        "    print(f'{count} n {filename} u {file}')\n",
        "    # os.system(f\"cat {file}/p* > {outputFoldermetadata}/{filename}.txt\")\n",
        "    # os.system(f\"ls {outputFoldermetadata}\")\n",
        "    \n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901306
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filelist = glob.glob('/tmp/workspacezone/tsv/*.txt')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputFoldermetadata = '/tmp/visit_document_note/tsv'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901339
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/tmp/visit_document_note/tsv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901355
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $outputFoldermetadata"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for count, file in enumerate(filelist):\n",
        "    filename = file.split('/')[4]\n",
        "    print(f'{count} n {filename} u {file}')\n",
        "    # os.system(f\"cat {file}/p* > {outputFoldermetadata}/{filename}.txt\")\n",
        "    # os.system(f\"ls {outputFoldermetadata}\")\n",
        "    \n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684793901375
        }
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "#Initiate Spark\n",
        "\n",
        "# Enter RAM of current compute\n",
        "ram = 2005\n",
        "driver_mem = int(ram * 0.9)\n",
        "cores = 128\n",
        "partitions = cores * 3\n",
        "\n",
        "print(\"Connect to Spark\")\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"SparkApp\") \\\n",
        "    .master('local[*]') \\\n",
        "    .config('spark.driver.memory', f'{driver_mem}g') \\\n",
        "    .config('spark.sql.shuffle.partitions', f'{partitions}') \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "    .config(\"spark.executor.memory\",\"8g\") \\\n",
        "    .config(\"spark.executor.cores\",\"1\") \\\n",
        "    .config(\"spark.python.worker.memory\",\"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"spark con\", spark)\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "print(spark.sparkContext.getConf().getAll())"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "spark"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "import webbrowser  \n",
        "\n",
        "weburl = sc.uiWebUrl\n",
        "vm_name = weburl.split('//')[1].split('.')[0]\n",
        "port = weburl.split(':')[-1]\n",
        "spark_ui = f'https://{vm_name}-{port}.usgovvirginia.instances.ml.azure.us/'\n",
        "webbrowser.open(spark_ui, new=0)\n",
        "print(f'Spark Web UI URL is: {spark_ui}')"
      ],
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "print('done')\n",
        "print('time at stage {:.6}'.format(time.time() - main_start))"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "from pyspark import SparkContext\n",
        "import pyspark"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "list(dataset_dict)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/workspacezone/TIU_visit_document_note_noXPNA"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "tiu_doc = spark.read.format(\"parquet\").load('/tmp/workspacezone/TIU_visit_document_note_noXPNA')"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "inputWidgets": {},
          "nuid": "facd6a95-ea75-4490-9a6d-ff2c0972119c",
          "showTitle": false,
          "title": ""
        }
      }
    },
    {
      "cell_type": "raw",
      "source": [
        "tiu_doc = spark.read.format('parquet').option(\"inferSchema\", \"true\").option(\"header\", \"true\") \\\n",
        "            .load('/tmp/workspacezone/TIU_2022_08_visit_document_note_noXPNA').na.drop(\"all\")"
      ],
      "metadata": {}
    },
    {
      "cell_type": "raw",
      "source": [
        "tiu_doc.count()"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/workspacezone/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "TIU_pull_formask_extended",
      "notebookOrigID": 864597534935248,
      "widgets": {}
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}